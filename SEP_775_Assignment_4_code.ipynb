{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bglmj68WjOd9"
   },
   "source": [
    "#**SEP 775 Assignment - 4**\n",
    "                        \n",
    "**Student: Rutvik Roy**\\\n",
    "**Student ID: 400490159**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tH3xzST72Y9o",
    "outputId": "fe3bf268-087a-4405-a945-682fec0ff8c7"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install peft\n",
    "!pip install tensorboard\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeFl5pkZqWoS",
    "outputId": "ff360927-4327-40f8-b599-fc4ed820b5dc"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.21.0\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gp6G3N9Of89o"
   },
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric,Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuD9QV--Ftia"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "0d4a286a26d445d48d20f13e6daaa16c",
      "bc2a097cc53e40fbb26608f76403a568",
      "bec236dce9f14abd9c0dc0da505e2351",
      "af9087579bde4682bff506aa55a76a6a",
      "8e26228a9aed426fbf116acb69958970",
      "1f6bb39b3e8241938bc712fec569152e",
      "63a307d7154f4d42b2752847ada4c667",
      "6623b4ba4be344d5b1a4b888e5c5d9d0",
      "3393f9c268414dbfb1edfdd7ff2d6912",
      "45a64dbbf64c4b91a18ba91131adfc92",
      "37f2388324e2497aac5103a4c565fc46",
      "a8e325ef094f4321bab984bf07d499a9",
      "f67aceb1d06d44b6aa92080846eab6b9",
      "837e584359374c3a8b6b5dd0b98788e1",
      "97b5f878ff1541c183e62c02b41c15a5",
      "4051e32e9e1743c9a22a7b68581f5380",
      "bcd8cf2b1a1b4e688a5ab0cef9824980",
      "e6d90ea355db468fb756e797d32d1826",
      "034f6c85da9747829456a7fd62a2bc0a",
      "3130dd3515234d7093fd682623491cfa",
      "218a3c45f71e4fb6863ccaebe8bd8463",
      "c2f25d23a2ee4461a3551c6c7e9ddf1b",
      "d24a36cdce4c4f5cae420f44f38f27e3",
      "862d0dc73b7a421fb6da401aadee4105",
      "ead2827467694df593216ba0d5e505d5",
      "c690521cdfdb4e3aba12ba7c05634ab3",
      "1a57fa103c0244e98edc8bacccacb145",
      "448f0b1faef64c5a8e336576781738ff",
      "622fc573e13e48d69345f096a1b6014c",
      "bf55be13873c4d5e9053e3f39ba91fc8",
      "5f5ed9299f2f4893b06b52636dab85a7",
      "1083407c271f40f78bf7d6501c5060cb",
      "14274d6acf3f45529ba03233ba050990",
      "8a6e4ff6efb248ddacf91593d84ee903",
      "31dc2a0ef0594fd584f3e6c7c63a62cf",
      "0e988b1b30d14aa09a802dc6e41e1ff3",
      "a79433679a47416a9044720d1bc2210b",
      "dab52ad010e54289a44c7effff23477b",
      "8a7bb1d9d4134642b14efeaf37799620",
      "0e5154644b9e43be97024abd9e107228",
      "bb6442656e6c47e58b88a321b9b31a22",
      "122333ebb21b44d391b19c3dc6b6daf0",
      "ca47518a71de43c5bf898a1c78c5ea6f",
      "e7b94febb8f7431d8416ca5e043b9796"
     ]
    },
    "id": "oywhQ7OS2uoM",
    "outputId": "00e6f34a-fa1c-4727-c29d-a9e659edaa07"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset=load_dataset(\"flytech/python-codes-25k\",split=\"train[:100]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "Iz9Ig-3dneTY",
    "outputId": "3ae8b46d-45f0-4764-ff6c-8b173c08b517"
   },
   "outputs": [],
   "source": [
    "#dataset to dataframe\n",
    "\n",
    "dataset_df = pd.DataFrame(dataset)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Ls8GQjqiguxS",
    "outputId": "e3f002e5-2919-47ea-ebca-1edceec27c47"
   },
   "outputs": [],
   "source": [
    "# remove (python\\n) from the 'output' column\n",
    "\n",
    "dataset_df['output'] = dataset_df['output'].str.replace(\"python\\n\", \"\")\n",
    "\n",
    "# remove unnecessary string like quatations\n",
    "dataset_df['output'] = dataset_df['output'].str.replace('```', '')\n",
    "\n",
    "# keep only necessary columns only\n",
    "dataset_df=dataset_df[['instruction','output']]\n",
    "\n",
    "# remove duplicate entry\n",
    "dataset_df.drop_duplicates(['instruction','output'],inplace=True)\n",
    "\n",
    "# delete empty rows\n",
    "dataset_df.dropna(inplace=True)\n",
    "\n",
    "# select only 100 rows for less training time\n",
    "dataset_df=dataset_df[:100]\n",
    "\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-jM0TzZkAZw"
   },
   "outputs": [],
   "source": [
    "input_text=dataset_df.instruction.values\n",
    "output_code=dataset_df.output.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "063d34265c0e451ab6cbba9403e50c50",
      "c87d446b5987463ca06fd2564b20d024",
      "374afceaaeaf4bb682bf780674445908",
      "9736a0e885c9411bb4b1ff4d4f2b60aa",
      "8cd7d3997c1c40498773b9a8c602b95b",
      "bd2abb615853493b9746a32582e80441",
      "4de1e6b27ddc4410be9e7f0428d4f261",
      "2ff76f91f07c410baadbf859c8e45dec",
      "3f551a1f1dce44bc97e79d4ffe738325",
      "448ed3ef327247afb84ceaa221308305",
      "d50d9903e973425b90e759a096012118",
      "89e2cabd169543f1ab03f5025776b488",
      "d9fbcb884f7542f79c031efc4f7428fa",
      "dc2583fc4884442f84ba7c42d66f6d70",
      "224e1940e9f14820819caa08606272e7",
      "1bcf845d74e54433b632b9407240a4ab",
      "0266fb11e9e64ce8aad78dbe5a2e8f22",
      "a787ffb914d544aca9d3896a2d77382b",
      "85f7bd7b9d9f4656bccd7507a773e4be",
      "fdc13aa3f49449a3970d99d2e5a4e0ce",
      "a96ddbfae81c4f0f88c72dd390acdae5",
      "e6229fa0fa524da692c7f1deb4bbd82d",
      "4b5d5a06cdb642ea8812e0b7fc9e53d1",
      "2a561f2caeb54750b929a0c510031766",
      "1cf894e1cb044119bb2445baaf22b302",
      "ea1cde49d681409f9f8fec47b643dac8",
      "2237ea2218674e999c1c1d1344a94d69",
      "13ebcf872c7c45cfa38e7bd4438d47b0",
      "262f5963fbf441579c2b012b2d30fbaa",
      "cd38f0d833d548c5a2e49fecab1bbd85",
      "bfa9b09c8c774ca290b528291c55eb6f",
      "378922ca5c8741cf87bd5b5da77a644c",
      "fe5daf44debf4fcb85971500773f891f",
      "31ef5e2f70434962910e837de246ee0b",
      "356530b478dc40868a1ea8899f15a0e4",
      "7f5e3a364af343218620e4fd49b36e5d",
      "e53e90343b804231b7751bad24168069",
      "2a1efb1a501541db86778937f079ad53",
      "6280aab8470e47bd88a071fdaddc63fa",
      "3379775559294bb483c7cf9924faf8d0",
      "a40ea76517bd4d27ba81e00d23a7dd4c",
      "5d498ea6699a4d29be9a406df4d44a23",
      "b0da2e34ba854692a31f2d1a88da68ef",
      "2856072c5642484fbf97045c2f7587e1",
      "37af485e0ca44026ac88f8146bc7de4b",
      "b3417dbb25a34e7e85c10d7f3930971c",
      "98a07ae5af9841889850dd111b78b364",
      "bfb8226f498f42d7a57176551ef6e8f5",
      "9612d48804f04e55be36a402e931dfc7",
      "b5fdb933aae348489db7f8598bae2382",
      "ce908b11fb1d4778a4855ddbbf4bfcc3",
      "1e1953d689bb4ec3bfce6dab8f7a38d7",
      "0ad7ba5f9d8442fd8d3ffcdb71b08e45",
      "5a3bcdaa09ad4dd395ab7b1322861991",
      "e0f10a75b4d14254b96b41c1f5ee6bea"
     ]
    },
    "id": "rUHtWQpXdiw3",
    "outputId": "7f4a8d12-3303-4cfa-837a-b14bfc961d57"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# the tokenizer of pretained model T5-base\n",
    "model_name = 'Salesforce/codet5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAmRJGq2Wi5-"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "  return tokenizer.tokenize(text)\n",
    "\n",
    "def tokenized_text_id(tokenized_text):\n",
    "  return tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "def add_special_tokens(input_text):\n",
    "  return ['<CLS>']+input_text+['<SEP>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_F0Bvy3XL_P",
    "outputId": "3ecc35c7-bd96-4983-a9ea-58905a38bc56"
   },
   "outputs": [],
   "source": [
    "print(\"Original:\",input_text[0])\n",
    "print(\"Tokenized input:\",tokenizer.tokenize(input_text[0]))\n",
    "print(\"Tokenized input ids:\",tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input_text[0])))\n",
    "print(\" \")\n",
    "print(\"Original:\",output_code[0])\n",
    "print(\"Tokenized output:\",tokenizer.tokenize(output_code[0]))\n",
    "print(\"Tokenized output ids:\",tokenizer.convert_tokens_to_ids(tokenizer.tokenize(output_code[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNyJPca6YGKB"
   },
   "outputs": [],
   "source": [
    "input_ids=[]\n",
    "\n",
    "for input_txt in (input_text):\n",
    "    input_txt=tokenize_text(input_txt)\n",
    "    input_tokens=add_special_tokens(input_txt)\n",
    "    ids=tokenized_text_id(input_tokens)\n",
    "    input_ids.append(ids)\n",
    "\n",
    "output_ids=[]\n",
    "\n",
    "for output in (output_code):\n",
    "    output=tokenize_text(output)\n",
    "    output_tokens=add_special_tokens(output)\n",
    "    ids=tokenized_text_id(output_tokens)\n",
    "    output_ids.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-SD61FsZHRB",
    "outputId": "1ceebcd3-374a-4cc3-b616-11356cd1eb36"
   },
   "outputs": [],
   "source": [
    "input_legnths=[]\n",
    "for ids in input_ids:\n",
    "    input_legnths.append(len(ids))\n",
    "input_maximum_length=max(input_legnths)\n",
    "print(\"Maximum input length:\",input_maximum_length)\n",
    "\n",
    "\n",
    "output_legnths=[]\n",
    "for ids in output_ids:\n",
    "    output_legnths.append(len(ids))\n",
    "output_maximum_length=max(output_legnths)\n",
    "print(\"Maximum output length:\",output_maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0CyppVqjx-B"
   },
   "outputs": [],
   "source": [
    "# function to tokenize the data and do padding according to maximum length\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['instruction'], padding='max_length', truncation=True, max_length=19, return_tensors='pt')\n",
    "    inputs['labels'] = tokenizer(examples['output'], padding='max_length', truncation=True, max_length=166, return_tensors='pt')['input_ids']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIO5CYV6hGUH"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "\n",
    "train_df, temp_df = train_test_split(dataset_df, train_size=0.8, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbGjOzOmqWoa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjCkezjVqWoa"
   },
   "source": [
    "# **Impliment Baseline Model and LORA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ssApv60qWob"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "a9241c0a8239406fa2b0056ab1ee3d74",
      "2cb4323032c24d62abf86fbaf3c8f41b",
      "bc1b80a2d0b545789e276c7348978d88",
      "0bc66eaf35e046e9b8990b77bde7fa4e",
      "67989de907a241c09dc658fa6a222d51",
      "6a1a291e4a004df38f97e89ab9ffba5f",
      "886f14906ae54d9b8f18b777b048cb79",
      "e1ec6ce71ba44f0ea97b0906dd361521",
      "154cd2099fb34ebe80b561a2ed6295fd",
      "404fbdd8b49646078e2ca2e6deaa7b3b",
      "881bb6e59a184774afb6c2bc27b12324",
      "19ee4450b74c49e792e787589e95c44f",
      "b8dd4953744348939e766e8271cfb13f",
      "fa79c8b84c9d49b8885e5fc99ba992d1",
      "ba16785714044b6e8cb4f119229f9d80",
      "f5b8d1a3894b453c9e1b54c0a70b3be9",
      "2bdfbbd4f24e4060b6b7a2876afe92f3",
      "a8b1820986f6411c95b5d782ea512583",
      "9d33acac565e4a388531beb41c359960",
      "3fe6b786cd9b446aad722695365ae51a",
      "ceffe5b3d22b4c11930ef230c41114f9",
      "532074737559478281c93e4b3f7b6032"
     ]
    },
    "id": "9apYZNKZqWob",
    "outputId": "c84fed45-c648-4643-8e25-c09e7e5bac25"
   },
   "outputs": [],
   "source": [
    "from peft import PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = 'Salesforce/codet5-base'\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# freez the original or baseline model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmYNXc4qqWob",
    "outputId": "b8b99652-b9be-433b-abca-3eee81db89ad"
   },
   "outputs": [],
   "source": [
    "# LORA configuration to be adapted\n",
    "\n",
    "peft_config=LoraConfig(\n",
    "\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "\n",
    "    # the dimension of the low-rank matrices\n",
    "    r=4,\n",
    "\n",
    "    # the scaling factor for the low-rank matrices\n",
    "    lora_alpha=32,\n",
    "\n",
    "    # the dropout probability of the LoRA layers\n",
    "    lora_dropout=0.01,\n",
    "\n",
    "    target_modules=[\"k\",\"q\",\"v\",\"o\"],\n",
    ")\n",
    "\n",
    "peft_model=get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPliCDDyaYyp",
    "outputId": "11ddbcb7-a1b9-461a-a450-66908cf60bd3"
   },
   "outputs": [],
   "source": [
    "# verify trainable parameters using require_grad method\n",
    "\n",
    "trainable_params = 0\n",
    "all_param = 0\n",
    "\n",
    "for _, param in peft_model.named_parameters():\n",
    "\n",
    "    #adding parameter\n",
    "    all_param += param.numel()\n",
    "    \n",
    "    #adding parameters to trainable if they require a graident\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "\n",
    "print(f\"trainable params: {trainable_params}\")\n",
    "print(f\"all params: {all_param}\")\n",
    "print(f\"trainable: {100 * trainable_params / all_param:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "77a1ccc3cb734c6695ec38a040c9e457",
      "2b23b6dad33f4c35a66d3eb52d96b9d4",
      "ee369d96beb44f6ba967130ee3f90431",
      "5128dd5743f34f978c112488ee1491e8",
      "caa836c8db924f7786b45a3a94dbaace",
      "7d0727e76bad4c49bd5ba72ca475f07e",
      "eb8dd3e1363e4f06840d64f33a82da83",
      "1cfc2fe1348b45fb9fcc68607c4c9014",
      "0d43fa4b1bad40cf8566acd49faec240",
      "caf4f13ff9404aa396b712f4bc3b25b0",
      "19ce5cfac09649548c1ed6a633578fd7",
      "563650d0ad9e484d905faf29d8a93df3",
      "c757c4b8acd749e7a58feb39d04640cc",
      "7ea876bd16844b2fbb56e2cae5419553",
      "c38085a8654c4a738a18ce621181ceb4",
      "e8eb5ce1b7484d44995fd9271853aa6f",
      "63170c59de8c453ba23563ba5a93aa97",
      "1826e8b6319a4848b14f217efa91c2ea",
      "06aa368a17cc420fb5fa9e17d08e850e",
      "97711b5151354c498ca292a496e578fa",
      "a3e40056320c4ae3832d0a77648fdb5d",
      "5ec99aa557db4bdba04a2029d4a15e0b"
     ]
    },
    "id": "cCCiyAvOvNJr",
    "outputId": "0366a259-9c6c-4419-db32-102bb1153627"
   },
   "outputs": [],
   "source": [
    "# the data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n",
    "\n",
    "# dataset object for training\n",
    "training_data = Dataset.from_pandas(train_df)\n",
    "\n",
    "# tokenize the training data\n",
    "tokenized_training_data = training_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# dataset object for validation\n",
    "validation_data = Dataset.from_pandas(val_df)\n",
    "\n",
    "# tokenize the validation data\n",
    "tokenized_validation_data = validation_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwL2KmnUqWod",
    "outputId": "61f57630-5a42-46af-b423-dfcfad96ad83"
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJRfm5FeqWod",
    "outputId": "f2200eb7-a687-4f53-98a6-b6fd6d383f9b"
   },
   "outputs": [],
   "source": [
    "tokenized_training_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdClP54_411x",
    "outputId": "8c14b2f5-7e1e-4f05-a2ac-8b08f74d50f2"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"./LORA_model\"\n",
    "\n",
    "# training args\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\tper_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    learning_rate=0.001,\n",
    "    num_train_epochs=100,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=False,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# trainer instance\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_training_data,\n",
    "    eval_dataset=tokenized_validation_data,)\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "gpNpqGvp417R",
    "outputId": "c92af579-1633-40e9-b1b5-d9925fcf4f01"
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNEjhg6t7rCG",
    "outputId": "d6a1f62d-a3da-46c7-8e41-1f78b0b0d75c"
   },
   "outputs": [],
   "source": [
    "#save trained model\n",
    "\n",
    "import os\n",
    "output_dir=\"/content/LORA_model\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)\n",
    "  \n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdwwVds3qWof"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4kOwkrwqWog"
   },
   "source": [
    "# **Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmyyqKvqqWog",
    "outputId": "f27fe60c-3e72-46c1-dfa5-b8a9d5261d0f"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# load peft config for pre-trained checkpoint\n",
    "\n",
    "peft_model_id=\"/content/LORA_model\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load model and tokenizer\n",
    "\n",
    "trained_model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "trained_model = PeftModel.from_pretrained(trained_model, peft_model_id)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA0nASdxqWog"
   },
   "outputs": [],
   "source": [
    "# Testing with Prompts (these prompts or instructions have been taken from original flytech/python-25k training dataset)\n",
    "\n",
    "prompts = ['Help me set up my daily to-do list!', 'Create a shopping list based on my inputs!', 'Calculate how much time I spend on my phone per week!', \"Help me split the bill among my friends!\"]\n",
    "generated_code_list_1 = []\n",
    "\n",
    "# Generate code for each prompt\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=19, truncation=True).input_ids.to(trained_model.device)\n",
    "    generated_ids = trained_model.generate(input_ids=input_ids, max_length=166)\n",
    "    generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    generated_code_list_1.append(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOrFIvQjqWoh",
    "outputId": "7b3bfd6e-0498-4baa-c347-6888b43c8aaa"
   },
   "outputs": [],
   "source": [
    "print(generated_code_list_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXfIrTZf7ZYf"
   },
   "source": [
    "# 1. BLUE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0AAkob441-u",
    "outputId": "3aa0b0ee-29e5-442e-ef42-51c80ffdc6d4"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "\n",
    "# reference code or true code for the prompts\n",
    "reference_code =[\"tasks = []\\nwhile True:\\n    task = input('Enter a task or type 'done' to finish: ')\\n    if task == 'done': break\\n    tasks.append(task)\\nprint(f'Your to-do list for today: {tasks}')\\n\",\n",
    "                 \"shopping_list = {}\\nwhile True:\\n    item = input('Enter an item or type 'done' to finish: ')\\n    if item == 'done': break\\n    quantity = input(f'Enter the quantity for {item}: ')\\n    shopping_list[item] = quantity\\nprint(f'Your shopping list: {shopping_list}')\\n\",\n",
    "                 \"total_time = 0\\nfor i in range(1, 8):\\n    time = float(input(f'Enter phone usage in hours for day {i}: '))\\n    total_time += time\\nprint(f'You spend approximately {total_time} hours per week on your phone.')\\n\",\n",
    "                 \"total_bill = float(input('Enter the total bill amount: '))\\nfriends_count = int(input('Enter the number of friends: '))\\nper_person = total_bill / friends_count\\nprint(f'Each person should pay {per_person}')\\n\"]\n",
    "bleu_scores_1 = []\n",
    "\n",
    "# calculate BLEU scores for each prompt\n",
    "\n",
    "for generated, reference in zip(generated_code_list_1, reference_code):\n",
    "    generated_tokens = generated.split()\n",
    "    reference_tokens = reference.split()\n",
    "    bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "    bleu_scores_1.append(bleu_score)\n",
    "\n",
    "# print BLEU scores for each prompt\n",
    "for i, (prompt, bleu_score,code) in enumerate(zip(prompts, bleu_scores_1,generated_code_list_1)):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Generated Code: {code}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# calculate the average BLEU score\n",
    "average_bleu_score = sum(bleu_scores_1) / len(bleu_scores_1)\n",
    "print(\"Average BLEU Score of Lora model:\", average_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buU9HvUdfKTm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9vwOYLU7euD"
   },
   "source": [
    "# 2. ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNs2Mi7PqWor",
    "outputId": "49d681ff-32e4-41a1-e830-2f4e7422e869"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge=Rouge()\n",
    "rouge_scores_1 = []\n",
    "\n",
    "print(f\"Rouge Scores of Lora Model:\")\n",
    "print(\" \")\n",
    "\n",
    "# calculate Rouge scores for each prompt\n",
    "for generated, reference in zip(generated_code_list_1, reference_code):\n",
    "    rouge_score = rouge.get_scores([generated], [reference])\n",
    "    rouge_scores_1.append(rouge_score)\n",
    "\n",
    "# print Rouge scores for each prompt\n",
    "for i, (prompt, rouge_score) in enumerate(zip(prompts, rouge_scores_1)):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Rouge Score:\",rouge_score)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i97GSbpRfLic"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqMYIRl9qWos"
   },
   "source": [
    "# **Comparision with baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlnqSeBnqWot"
   },
   "outputs": [],
   "source": [
    "# load the baseline model which has been used for training\n",
    "\n",
    "model_name = \"Salesforce/codet5-base\"\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKX6zsRxqWot"
   },
   "outputs": [],
   "source": [
    "generated_code_list_2 = []\n",
    "\n",
    "# Generate code for each prompt using baseline model\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=38, truncation=True).input_ids.to(baseline_model.device)\n",
    "    generated_ids = baseline_model.generate(input_ids=input_ids,max_new_tokens=400)\n",
    "    generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    generated_code_list_2.append(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA3S6Qyx9sli"
   },
   "source": [
    "# 1. Baseline model BLUE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HcO3p4yqWot",
    "outputId": "e2cada06-8dec-46c7-914c-c535742cef62"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu_scores_2 = []\n",
    "\n",
    "# calculate BLEU scores for each prompt\n",
    "for generated, reference in zip(generated_code_list_2, reference_code):\n",
    "    generated_tokens = generated.split()\n",
    "    reference_tokens = reference.split()\n",
    "    bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "    bleu_scores_2.append(bleu_score)\n",
    "\n",
    "# print BLEU scores for each prompt\n",
    "for i, (prompt, bleu_score,code) in enumerate(zip(prompts, bleu_scores_2,generated_code_list_2)):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Generated Code: {code}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# calculate the average BLEU score\n",
    "average_bleu_score = sum(bleu_scores_2) / len(bleu_scores_2)\n",
    "print(\"Average BLEU Score of Baseline model:\", average_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nKOOn9IfNiW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnUq-laI91Db"
   },
   "source": [
    "# 2. Baseline model Rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKwEFLcbqWou",
    "outputId": "d7a21bf5-7cd2-435c-aeaa-ba65603eb768"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge=Rouge()\n",
    "rouge_scores_2 = []\n",
    "\n",
    "print(f\"Rouge Scores of Baseline Model:\")\n",
    "print(\" \")\n",
    "\n",
    "# calculate Rouge scores for each prompt\n",
    "for generated, reference in zip(generated_code_list_2, reference_code):\n",
    "    rouge_score = rouge.get_scores([generated], [reference])\n",
    "    rouge_scores_2.append(rouge_score)\n",
    "\n",
    "# print Rouge scores for each prompt\n",
    "for i, (prompt, rouge_score) in enumerate(zip(prompts, rouge_scores_2)):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Rouge Score:\",rouge_score)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAaaTABPfOeT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
